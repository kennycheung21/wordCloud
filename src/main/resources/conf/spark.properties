# spark conf properties
spark.driver.cores=1
spark.driver.memory=512m
spark.executor.memory=1g
spark.executor.cores=1
spark.master=yarn
spark.submit.deployMode=cluster

spark.driver.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
spark.eventLog.dir=hdfs:///spark2-history/
spark.eventLog.enabled=true
spark.executor.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
spark.history.fs.logDirectory=hdfs:///spark2-history/
spark.history.kerberos.keytab=none
spark.history.kerberos.principal=none
spark.history.provider=org.apache.spark.deploy.history.FsHistoryProvider
spark.history.ui.port=18081
spark.yarn.historyServer.address=s4.poc.support.com:18081
spark.yarn.queue=default

spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.maxExecutors=10
spark.dynamicAllocation.minExecutors=0

spark.streaming.backpressure.enabled=true
spark.streaming.kafka.maxRatePerPartition=20
spark.streaming.ui.retainedBatches=100
spark.shuffle.service.enabled=true
spark.streaming.kafka.consumer.poll.ms=2000
